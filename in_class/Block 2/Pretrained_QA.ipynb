{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffea51be",
   "metadata": {},
   "source": [
    "# Question-Answering\n",
    "Question answering is becoming more and more popular, as people seem to prefer to get answer interactively rather than reading long documents, and information can be found more quickly. \n",
    "\n",
    "We will the SubjQA dataset, which contains over 10'000 customer reviews in English  in six domains. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2944902",
   "metadata": {},
   "source": [
    "**Note:** If you get a `NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported.` when running the cell `print(subjqa[\"train\"][\"answers\"][1])`: uncomment the following two `!pip` commands, restart the kernel, and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72de8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U typing-extensions\n",
    "# !pip install fsspec==2023.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62d2212",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset_config_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368e4d6b",
   "metadata": {},
   "source": [
    "The `subjqa` has reviews from different domains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddbd99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = get_dataset_config_names(\"subjqa\", trust_remote_code=True)\n",
    "domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40b9e7",
   "metadata": {},
   "source": [
    "We will use the feedbacks on electronics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae8dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "subjqa = load_dataset(\"subjqa\", name=\"electronics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274261b",
   "metadata": {},
   "source": [
    "Let's first look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d22603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subjqa[\"train\"][\"answers\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73783d",
   "metadata": {},
   "source": [
    "We re-organize the data into a structure that is more easy to handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee69d6",
   "metadata": {},
   "source": [
    "And we want to get an overview over how many data points we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, df in dfs.items():\n",
    "    print(f\"Number of questions in {split}: {df['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc0d9c",
   "metadata": {},
   "source": [
    "Here are some of the most relevant attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_cols = [\"title\", \"question\", \"answers.text\",\n",
    "           \"answers.answer_start\", \"context\"]\n",
    "sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[791, 'context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24fbe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.loc[1159, 'context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4b47b-5556-4b6d-8cc4-511747d46ca7",
   "metadata": {},
   "source": [
    "**Exercise:** Now it is your turn. Inspect at least 2 sample rows in more detail to get a better feeling for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8225c2a2-7ceb-44ad-9d09-d6bf0000112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=10) # fill in this line two sample two rows\n",
    "your_sample_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b63d1-0ca7-4d1a-8b65-e48c4ae9e68f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to see the solution</summary>\n",
    "\n",
    "  ```python\n",
    "  # solution\n",
    "  your_sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=10)\n",
    "  your_sample_df.reset_index(inplace = True)\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005249b-5371-4260-9d62-35fd41bec9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(your_sample_df.loc[0, ...]) # extract the column \"context\" from the first row\n",
    "print(your_sample_df.loc[0, ...]) # extract the column \"question\" from the first row\n",
    "print(your_sample_df.loc[0, ...]) # extract the column \"answers.text\" from the first row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444054ad-8ba2-4de0-8e59-00edf67bbecc",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to see the solution</summary>\n",
    "\n",
    "  ```python\n",
    "# solution\n",
    "print(your_sample_df.loc[0, \"context\"])\n",
    "print(your_sample_df.loc[0, \"question\"])\n",
    "print(your_sample_df.loc[0, \"answers.text\"])\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155d56f-1fb6-4eb1-834b-308fe22fb60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(your_sample_df.loc[..., ...]) # extract the column \"context\" from the second row\n",
    "print(your_sample_df.loc[..., ...]) # extract the column \"question\" from the second row\n",
    "print(your_sample_df.loc[..., ...]) # extract the column \"answers.text\" from the second row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19014c5b-c4f1-4aa7-8b8f-d178f995c089",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to see the solution</summary>\n",
    "\n",
    "  ```python\n",
    "# solution\n",
    "print(your_sample_df.loc[1, \"context\"])\n",
    "print(your_sample_df.loc[1, \"question\"])\n",
    "print(your_sample_df.loc[1, \"answers.text\"])\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a64f5",
   "metadata": {},
   "source": [
    "As mentioned in the slides, we see that some of the answers are grammatically not correct sentences, and some cannot be answered (such as \"how is the battery?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd8fd7",
   "metadata": {},
   "source": [
    "## Span Classification\n",
    "A popular approach is to define a task to find the start and end token of the context that is contains the answer.\n",
    "\n",
    "Many models for this are available on e.g., Huggingface Hub. We choose a small one by deepset, a German AI start-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb291502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002ae04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How much music can this hold?\"\n",
    "context = \"\"\"An MP3 is about 1 MB/minute, so about 6000 hours depending on file size.\"\"\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3297d47",
   "metadata": {},
   "source": [
    "As before, we can get the tokens that the tokenizer has generated - and with `tokenizer.decode`, we see how our question and context are represented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e05c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc93f39",
   "metadata": {},
   "source": [
    "After tokenization, we can use the model to predict the start and end token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1409c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0460855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a83ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "s_scores = start_logits.detach().numpy().flatten()\n",
    "e_scores = end_logits.detach().numpy().flatten()\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "token_ids = range(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2451707",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True)\n",
    "colors = [\"C0\" if s != np.max(s_scores) else \"C1\" for s in s_scores]\n",
    "ax1.bar(x=token_ids, height=s_scores, color=colors)\n",
    "ax1.set_ylabel(\"Start Scores\")\n",
    "colors = [\"C0\" if s != np.max(e_scores) else \"C1\" for s in e_scores]\n",
    "ax2.bar(x=token_ids, height=e_scores, color=colors)\n",
    "ax2.set_ylabel(\"End Scores\")\n",
    "plt.xticks(token_ids, tokens, rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe75ee-00aa-49ee-90fe-880545875955",
   "metadata": {},
   "source": [
    "Putting the components together, we can now get the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fe4fc-8574-492e-ac18-0f2a2b9e9833",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = torch.argmax(start_logits)\n",
    "end_idx = torch.argmax(end_logits) + 1\n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca3c488-2bf9-42a3-8904-d1df222ad9d9",
   "metadata": {},
   "source": [
    "**Exercise:** Now it is your turn. Provide a context and question of your choice. Use the model to answer that question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3608a989-d340-4d2d-b004-449f9c133f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_question = ... # fill in this line\n",
    "your_context = ... # fill in this line\n",
    "\n",
    "your_inputs = tokenizer(..., ..., return_tensors=\"pt\") # fill in this line\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(...) # fill in this line\n",
    "\n",
    "start_logits = ... # fill in this line\n",
    "end_logits = ... # fill in this line\n",
    "start_idx = ...# fill in this line\n",
    "end_idx = ...# fill in this line\n",
    "answer_span = ...# fill in this line\n",
    "answer = ...# fill in this line\n",
    "\n",
    "print(f\"Question: {your_question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e480118-7d05-4c98-b97b-188991ce1a5c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click to see the solution</summary>\n",
    "\n",
    "  ```python\n",
    "# solution\n",
    "your_question = \"How many servings does this recipe make?\" # possible solution\n",
    "your_context = \"\"\"\n",
    "This recipe for spaghetti bolognese uses 500 grams of pasta and 1 kilogram of vegan beef,\n",
    "along with other ingredients. It is designed to serve 4 people, with each serving\n",
    "being approximately 125 grams of pasta and 250 grams of vegan beef.\n",
    "\"\"\" # possible solution\n",
    "your_inputs = tokenizer(your_question, your_context, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(**your_inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "start_idx = torch.argmax(start_logits)\n",
    "end_idx = torch.argmax(end_logits) + 1\n",
    "answer_span = your_inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "answer = tokenizer.decode(answer_span)\n",
    "\n",
    "print(f\"Question: {your_question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "  ```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ba64e-021c-43cb-8f5f-d563336f6160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
